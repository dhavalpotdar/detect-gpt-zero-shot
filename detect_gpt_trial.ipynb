{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c46f33-ff3f-4f05-aed0-02358d314665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = 'mps'\n",
    "BASE_MODEL_NAME = 'gpt2-medium'\n",
    "MASK_FILLING_MODEL_NAME = 't5-large'\n",
    "PROMPT_TOKENS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48acf3d0-4121-4ddf-b185-5f0952262100",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = \\\n",
    "''''Maj Richard Scott, 40, is accused of driving at speeds of up to 95mph (153km/h) in bad weather before the smash \n",
    "on a B-road in Wiltshire. Gareth Hicks, 24, suffered fatal injuries when the van he was asleep in was hit by Mr Scott\\'s \n",
    "Audi A6. Maj Scott denies a charge of causing death by careless driving. Prosecutor Charles Gabb alleged the defendant, \n",
    "from Green Lane in Shepperton, Surrey, had crossed the carriageway of the 60mph-limit B390 in Shrewton near Amesbury. \n",
    "The weather was \"awful\" and there was strong wind and rain, he told jurors. He said Mr Scott\\'s car was described as \n",
    "\"twitching\" and \"may have been aquaplaning\" before striking the first vehicle; a BMW driven by Craig Reed. Mr Scott\\'s \n",
    "Audi then returned to his side of the road but crossed the carriageway again before colliding head-on with a Ford Transit \n",
    "van in which Mr Hicks was a passenger, the court was told. \"There is no doubt that when the Audi smashed into the panel \n",
    "van he was on completely the wrong side of the road,\" Mr Gabb said. Mr Hicks, from Bath in Somerset, was asleep in the \n",
    "van being driven to a construction site in Salisbury by fellow DR Groundworks colleague, Patrick Gilleece. The jury was \n",
    "told the Maj Scott suffered \"substantial injuries\" and could not recall the crash, which happened shortly after 07:00 GMT \n",
    "on 6 October, 2014. He does not accept the charge and suggests it was in fact Mr Reed who had crossed the carriageway, \n",
    "causing the collision, Mr Gabb told the court. The trial continues.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55dacc34-cf5b-4937-bbe8-f1c09b7399c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model():\n",
    "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "    mask_model.cpu()\n",
    "    base_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "def load_mask_model():\n",
    "    print('MOVING MASK MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "    mask_model.to(DEVICE)\n",
    "    base_model.cpu()\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "def trim_to_shorter_length(texta, textb):\n",
    "    # truncate to shorter of o and s\n",
    "    shorter_length = min(len(texta.split(' ')), len(textb.split(' ')))\n",
    "    texta = ' '.join(texta.split(' ')[:shorter_length])\n",
    "    textb = ' '.join(textb.split(' ')[:shorter_length])\n",
    "    return texta, textb\n",
    "\n",
    "def get_ll(text):\n",
    "    with torch.no_grad():\n",
    "        tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "        return -base_model(**tokenized, labels=labels).loss.item()\n",
    "\n",
    "\n",
    "def tokenize_and_mask(text, span_length, pct, ceil_pct=False):\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    n_spans = pct * len(tokens) / (span_length + 1 * 2)\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - 1)\n",
    "        search_end = min(len(tokens), end + 1)\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def count_masks(texts):\n",
    "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "\n",
    "def replace_masks(texts):\n",
    "    n_expected = count_masks(texts)\n",
    "    stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "    tokens = mask_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs = mask_model.generate(**tokens, max_length=150, do_sample=True, top_p=1, num_return_sequences=1, eos_token_id=stop_id)\n",
    "    return mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "def extract_fills(texts, pattern=re.compile(r\"<extra_id_\\d+>\")):\n",
    "    # remove <pad> from beginning of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # return the text in between each matched mask token\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # remove whitespace around each fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "def apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # replace each mask token with the corresponding fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # join tokens back into text\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n",
    "\n",
    "def generate_perturbed_texts(text, n):\n",
    "    perturbed_texts = []\n",
    "    for _ in range(n):\n",
    "        masked_sample = tokenize_and_mask(text, span_length=2, pct=0.3)\n",
    "        raw_fills = replace_masks([masked_sample])\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        perturbed_text = apply_extracted_fills([masked_sample], extracted_fills)\n",
    "        perturbed_texts.append(perturbed_text)\n",
    "        pass\n",
    "    return perturbed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41eb774-2e6d-4213-a00e-a27a666bdde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhavalpotdar/.pyenv/versions/3.10.0/envs/ids703-nlp-detectgpt/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token # throws error if not set\n",
    "\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(MASK_FILLING_MODEL_NAME)\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(MASK_FILLING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c518c4f-42cf-48ec-aac6-3709747ccbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (0.87s)\n"
     ]
    }
   ],
   "source": [
    "load_base_model()\n",
    "\n",
    "# tokenize text and slice the first PROMPT_TOKENS\n",
    "all_encoded = base_tokenizer(real_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "all_encoded = {key: value[:, :PROMPT_TOKENS] for key, value in all_encoded.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0d2113-0d51-4208-a39f-810d1b0b0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_kwargs = dict()\n",
    "sampling_kwargs['top_p'] = 0.96\n",
    "sampling_kwargs['top_k'] = 40\n",
    "min_length = 50\n",
    "max_length = 200\n",
    "\n",
    "outputs = base_model.generate(**all_encoded, \n",
    "                              min_length=min_length, \n",
    "                              max_length=max_length, \n",
    "                              do_sample=True, \n",
    "                              **sampling_kwargs, \n",
    "                              pad_token_id=base_tokenizer.eos_token_id, \n",
    "                              eos_token_id=base_tokenizer.eos_token_id)\n",
    "sampled_text = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "sampled_text = sampled_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa8c5e7-4de2-4457-bc12-09b91de1bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['real'], data['sampled'] = trim_to_shorter_length(real_text, sampled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33dfc7a1-0f5a-4372-a00f-05ff1ceec373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.108694314956665, -1.8114635944366455)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get likelihood of each text under base model\n",
    "ll_real = get_ll(data['real'])\n",
    "ll_sampled = get_ll(data['sampled'])\n",
    "\n",
    "(ll_real, ll_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8c5d0e-be98-4335-8320-19e1229a418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING MASK MODEL TO GPU...DONE (2.11s)\n"
     ]
    }
   ],
   "source": [
    "load_mask_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7df63f9-d987-49e7-adb4-0e72c942d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_texts = generate_perturbed_texts(data['sampled'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "849de7a4-4930-4f74-8960-0896668b239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVING BASE MODEL TO GPU...DONE (4.49s)\n"
     ]
    }
   ],
   "source": [
    "load_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed754b23-e79a-45f0-bf2b-0515e5aaa9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lls_perturbed = torch.tensor([get_ll(perturbed_text) for perturbed_text in perturbed_texts])\n",
    "\n",
    "MU = (1/10) * torch.sum(lls_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ae44cc0-2e3e-4743-bd32-0cd27f427a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_norm = torch.sqrt((1/(10 - 1)) * torch.square(torch.sum(lls_perturbed - MU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23b5d05a-47ef-437e-9153-7165a6d110b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3447e-07)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f7a4562-6c6b-461e-afdc-6bafb20f506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1380807.2500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ll_real - MU)/variance_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
